---
title: "Learning Curves"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
    social: menu
    source_code: https://github.com/bradleyboehmke/learningCurve-shiny-app
    theme: simplex
runtime: shiny
---

```{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(learningCurve)
library(ggplot2)
library(plotly)
library(DT)
```


Sidebar {.sidebar}
======================================================================

```{r}

# Define inputs

textInput("unit_a", "Starting Unit", 1)
textInput("time_a", "Starting Unit Time", 100)
textInput("unit_b", "Ending Unit", 250)

sliderInput("rate", "Learning Curve Rate:",
    min = 0, max = 1, value = .8
  )

radioButtons("model", "Model:",
             c("Crawford's Unit Learning Curve Function" = "u",
               "Wright's Cumulative Average Learning Curve Function" = "ca"))

#actionButton("goButton", "Compute!")

```

<br>

Use the __Explore Learning Curves__ tab to explore how differing inputs such as starting unit time, learning rate, and type of learning curve function (Crawford's unit function versus Wright's cummulative average function) influences the time (or cost) predicted for the ending unit. Scroll along the unit- and cummulative-level plots to examine predicted time (or cost) for different levels of units produced.  

To learn more click the __About__ tab.  

Application author: [Bradley Boehmke](http://bradleyboehmke.github.io/)


Explore Learning Curves
======================================================================

Row
-----------------------------------------------------------------------

### Unit-level Estimates

```{r}

df <- reactive({
  if(input$model == "u"){
    
  data.frame(Unit = as.numeric(input$unit_a):as.numeric(input$unit_b),
             Time = unit_curve(t = as.numeric(input$time_a),
                               m = as.numeric(input$unit_a),
                               n = as.numeric(input$unit_a):as.numeric(input$unit_b),
                               r = as.numeric(input$rate)),
             Cummulative = cumsum(unit_curve(t = as.numeric(input$time_a),
                                             m = as.numeric(input$unit_a),
                                             n = as.numeric(input$unit_a):as.numeric(input$unit_b),
                                             r = as.numeric(input$rate))))
  } else {
    
    data.frame(Unit = as.numeric(input$unit_a):as.numeric(input$unit_b),
             Time = ca_unit(t = as.numeric(input$time_a),
                            m = as.numeric(input$unit_a),
                            n = as.numeric(input$unit_a):as.numeric(input$unit_b),
                            r = as.numeric(input$rate)),
             Cummulative = cumsum(ca_unit(t = as.numeric(input$time_a), 
                                          m = as.numeric(input$unit_a),
                                          n = as.numeric(input$unit_a):as.numeric(input$unit_b), 
                                          r = as.numeric(input$rate))))
  }
})
```


```{r}
renderPlotly({
  p1 <- ggplot(df(), aes(x = Unit, y = Time)) +
    geom_line() +
    scale_x_continuous("Units Produced", labels = scales::comma) +
    scale_y_continuous("Per Unit Time (or Cost)", labels = scales::comma)
  
  ggplotly(p1)
})
```

### Cummulative-level Estimates

```{r}

renderPlotly({
  p1 <- ggplot(df(), aes(x = Unit, y = Cummulative)) +
    geom_line() +
    scale_x_continuous("Units Produced", labels = scales::comma) +
    scale_y_continuous("Cummulative Time (or Cost)", labels = scales::comma)
})
```

Row
-----------------------------------------------------------------------

### Model Definition

Crawford's unit learning curve model:

$$y_n = t_m * \left(\frac{n}{m}\right)^b$$

Wright's cummulative average learning curve model:

$$y_{n} = t_{m} * \frac{n^{1+b} - (n-1)^{1+b}}{m^{1+b} - (m-1)^{1+b}}$$

both where

&nbsp;&nbsp;&nbsp;&nbsp; $y_n$ = the time (or cost) required for the nth unit of production <br>
&nbsp;&nbsp;&nbsp;&nbsp; $t_{m}$ = time (or cost) required for the mth unit of production <br> 
&nbsp;&nbsp;&nbsp;&nbsp; *m* = mth unit of production <br>
&nbsp;&nbsp;&nbsp;&nbsp; *n* = nth unit you wish to predict the time (or cost) for <br>
&nbsp;&nbsp;&nbsp;&nbsp; *b* = natural slope of the learning curve rate <br>


To compute *b* for a given learning rate:
  
$$b = \frac{log(r)}{log(2)}$$





### Underlying Data

```{r}
df.table <- reactive({
  if(input$model == "u"){
    
  data.frame(Unit = as.numeric(input$unit_a):as.numeric(input$unit_b),
             `Unit Time` = round(unit_curve(t = as.numeric(input$time_a),
                               m = as.numeric(input$unit_a),
                               n = as.numeric(input$unit_a):as.numeric(input$unit_b),
                               r = as.numeric(input$rate)), 0),
             `Cummulative Time` = round(cumsum(unit_curve(t = as.numeric(input$time_a),
                                             m = as.numeric(input$unit_a),
                                             n = as.numeric(input$unit_a):as.numeric(input$unit_b),
                                             r = as.numeric(input$rate))), 0))
  } else {
    
    data.frame(Unit = as.numeric(input$unit_a):as.numeric(input$unit_b),
             `Unit Time` = round(ca_unit(t = as.numeric(input$time_a),
                            m = as.numeric(input$unit_a),
                            n = as.numeric(input$unit_a):as.numeric(input$unit_b),
                            r = as.numeric(input$rate)), 0),
             `Cummulative Time` = round(cumsum(ca_unit(t = as.numeric(input$time_a), 
                                          m = as.numeric(input$unit_a),
                                          n = as.numeric(input$unit_a):as.numeric(input$unit_b), 
                                          r = as.numeric(input$rate))), 0))
  }
})

renderDataTable({
  datatable(df.table(), rownames = FALSE,
            colnames = c('Unit Number', 'Unit Time', 'Cummulative Time'),
            options = list(pageLength = 8, dom = 'tip'))
})
```







About
======================================================================

This application is in support of the article in _Urban Studies_, ["Locating neighborhood diversity in the American Metropolis."](http://usj.sagepub.com/content/early/2016/04/29/0042098016643481.abstract)  The article analyzes geographic variations in neighborhood racial and ethnic diversity over time in large metropolitan areas in the United States.  

The key metric in this article is the neighborhood-level _entropy index_ (called "diversity score" in the application), which measures the degree of neighborhood diversity for four general racial/ethnic groups: non-Hispanic white, non-Hispanic black, Hispanic, and Asian/Pacific Islander.  The entropy index $E$ is calculated as follows (Farrell and Lee 2011):  

$$E = {\sum\limits_{r=1}^{n}Q_r}ln{\dfrac{1}{Q_r}}$$

where $Q_r$ is group $r$'s proportion of the neighborhood population.  The maximum value of $E$, then, is the natural log of the number of groups - which would occur when all groups in a neighborhood are of equal size. Following [Hall and Lee (2010)](http://usj.sagepub.com/content/47/1/3.abstract), [Farrell and Lee (2011)](http://www.sciencedirect.com/science/article/pii/S0049089X11000706), and [Wright et al. (2014)](http://www.tandfonline.com/doi/abs/10.1080/00330124.2012.735924#.Vwxi7fkrLRY), $E$ is scaled by its maximum by dividing by $ln(4)$, setting the range of values from 0 to 1.  

To study how neighborhood diversity varies with distance from urban cores in the largest metropolitan areas in the United States, entropy indices are plotted against the distance from the Census tract centroids to their corresponding nearest major city hall.  Locally-weighted regression (LOESS) is then used to produce a "diversity gradient" of estimates of neighborhood diversity by distance from the city center.  

This application allows visitors to explore this part of the paper interactively.  The article follows by using local exploratory spatial data analysis techniques to identify how spatial clusters of diversity have shifted over time; this will be the focus of a future application that corresponds to an extension of the study published in _Urban Studies._  

Demographic data for the paper come from [Brown University's Longitudinal Tract Database](http://www.s4.brown.edu/us2010/Researcher/LTDB.htm), and geographic data in the paper come from the [National Historical Geographic Information System](https://www.nhgis.org/).  Geographic data in the application are from the [US Census Bureau's Cartographic Boundary Files](https://www.census.gov/geo/maps-data/data/tiger-cart-boundary.html) and subsequently simplified with the [rmapshaper](https://github.com/ateucher/rmapshaper) R package to improve performance.  

The application is built with the [Shiny](http://shiny.rstudio.com) framework for the [R programming language](https://www.r-project.org/). The application layout is produced with the [flexdashboard](http://rstudio.github.io/flexdashboard/index.html) package, and the charts and maps use [Plotly](http://plot.ly), [Leaflet.js](http://leafletjs.com/), [Highcharts](http://www.highcharts.com/), and [ggplot2](http://ggplot2.org/), all accessed through their corresponding R packages.  

Research code for the article is available upon request, and will be formally released this summer as a GitHub repository.  Code for the application is available at <https://github.com/walkerke/neighborhood_diversity>.  

I welcome feedback and suggestions!  [Please visit my personal website](http://personal.tcu.edu/kylewalker/) for contact information or [connect with me on Twitter](https://twitter.com/kyle_e_walker).  

